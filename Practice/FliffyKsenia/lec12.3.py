import re
from urllib.request import urlopen

url = "http://www.google.com"                # адрес на котором ищем все ссылки
page = urlopen(url)                          # получаем страницу
find_href = r'(<a[^>]href=\")(\S*)(\")'      # вводим регулярное выражение для поиска ссылок
urls = [[]]                                  # объявляем пустой массив для списка урл
html_bytes = page.read()                     # загружаем страницу в переменную html_bytes
html = html_bytes.decode("utf-8")            # декодируем страницу в кодировку utf-8
hrefs = re.findall(find_href, html)          # применяем регуляроное выражение и ищем все совпадения.
for ur in hrefs:                             # обходим результаты работы модуля re
    u = ur[1]                                # так как у нас re возвращает tuple, то его изменять нельзя. поэтому вводим значение во временную переменную
    if u[0] == "/":                          # проверяем внутренняя ли ссылка.
        u = url + u                          # если была, внутренняя, то делаем из нее внешнюю.
    is_avail = False                         # вводим переменную доступности
    try:
        p = urlopen(u)                       # проверяем на досупность ссылки в инернете
        is_avail = True                      # если мы дошли до этой строчки, то значит ссылка досутупна
    except:
        pass                                 # пустой оператор, чтобы программы не вылетала
    urls.append([u, is_avail])               # дописываем ссылку в массив ссылок и результат ее проверки

print("Живость ссылки  | Ссылка")            # выводим на экран получившийся массыв ссылок
urls.pop(0)                                  # удаляем пустой массив
for u in urls:                               # обходим массыв ссылок
    if u[1]:                                 # проверяем параметр доступности
        print(f"ссылка досупна  | {u[0]}")   # выводим сообщение если ссылка достуна
    else:
        print(f"ссылка недосупна| {u[0]}")   # выводим если ссылка недоступна
